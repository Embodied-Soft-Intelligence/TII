<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer</title>

  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <meta name="description" content="D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer" />

  <style>
    :root{--accent:#6366f1;--muted:#6b7280}
    html,body{height:100%}
    body{
      font-family:Inter,ui-sans-serif,system-ui,-apple-system,"Segoe UI",Roboto,'Helvetica Neue',Arial;
      background:linear-gradient(180deg,#fbfdff 0%, #f3f6ff 100%);
      color:#0f172a
    }
    .container{max-width:1200px}
    .shadow-soft{box-shadow:0 8px 30px rgba(15,23,42,0.06)}
    .glass{background:rgba(255,255,255,0.75);backdrop-filter:blur(6px)}
    .figure-img{max-height:520px;object-fit:contain}
    .caption{color:var(--muted);font-size:0.95rem}
    p, td, th, .text-justify {text-align: justify;}

    .uniform-width{
      width:100%;
      max-width:1000px;
      margin-left:auto;
      margin-right:auto;
    }

    table{min-width:100%;white-space:nowrap;font-size:0.8rem}
    th,td{padding:6px 8px;text-align:center}
    table th{font-size:0.75rem;font-weight:600;background:#fbfbff}

    img,video{outline:none;border:none}
    .image-container{border-radius:0.5rem;overflow:hidden}
  </style>
</head>

<body class="antialiased text-slate-900">

<!-- Top bar -->
<nav class="border-b bg-white/60 sticky top-0 backdrop-blur z-40">
  <div class="container mx-auto px-5 py-3 flex items-center justify-between">
    <div class="flex items-center gap-3">
      <div class="w-10 h-10 rounded-lg bg-gradient-to-br from-indigo-500 to-purple-500 flex items-center justify-center text-white font-bold shadow-soft">D2</div>
      <div>
        <div class="text-sm font-semibold">D2TriPO-DETR</div>
        <div class="text-xs text-gray-500">Dual-Decoder Triple-Parallel-Output Detection Transformer</div>
      </div>
    </div>
    <div class="hidden md:flex items-center gap-6 text-sm text-slate-700">
      <a href="#fig1" class="hover:underline">Schematic</a>
      <a href="#results" class="hover:underline">Results</a>
      <a href="#summary" class="hover:underline">Real-World Experiment</a>
      <a href="#video" class="hover:underline">Video</a>
    </div>
  </div>
</nav>

<header class="container mx-auto px-5 py-10 text-center">
  <h1 class="text-3xl sm:text-4xl md:text-5xl font-extrabold leading-tight">
    D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer
  </h1>

  <div class="mt-6 flex justify-center">
    <a href="https://github.com/Embodied-Soft-Intelligence/TII"
       class="px-6 py-3 bg-indigo-500 text-white rounded-lg font-medium hover:bg-indigo-600 transition-colors">
      Code
    </a>
  </div>

  <p class="mt-6 text-gray-700 leading-relaxed glass p-4 rounded shadow-soft text-justify uniform-width">
    Vision-based grasping, though widely employed for industrial and household applications, still struggles with
    object stacking scenarios. Current methods face three major challenges:
    <strong>1)</strong> limited inter-object relationship understanding,
    <strong>2)</strong> poor grasping adaptation across different viewpoints, and
    <strong>3)</strong> error propagation.
    Inspired by distributed perception and visual adaptation from the human visual attention system,
    we propose <strong>D2TriPO-DETR</strong>, a dual-decoder transformer with triple parallel outputs
    (object detection, manipulation relationship, and grasp detection).
    The evaluation on the Visual Manipulation Relationship Dataset demonstrates consistent improvements,
    and real-world testing confirms its effectiveness in stacked-object grasping.
  </p>
</header>

<main class="container mx-auto px-5 pb-16">

<!-- Fig 1 -->
<section id="fig1" class="mt-8 text-center">
  <div class="inline-block bg-white rounded-lg p-4 shadow-soft uniform-width image-container">
    <img src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/1.png"
         class="w-full figure-img rounded"
         alt="Fig.1 Architecture">
    <p class="mt-2 text-sm text-gray-600">
      Human visual attention characteristics and overall schematics of D2TriPO-DETR.
    </p>
  </div>
</section>

<!-- Fig 3 -->
<section class="mt-8 text-center">
  <div class="inline-block bg-white rounded-lg p-4 shadow-soft uniform-width image-container">
    <img src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/3.png"
         class="w-full figure-img rounded"
         alt="Fig.3 Architecture">
    <p class="mt-2 text-sm text-gray-600">
      Detailed architecture of D2TriPO-DETR and its key modules.
    </p>
  </div>
</section>

<!-- Experiment setup -->
<section class="mt-12 uniform-width">
  <h3 class="font-semibold text-xl text-center mb-6">Experiment Setup</h3>

  <div class="glass p-6 rounded-lg shadow-soft">
    <h4 class="font-semibold text-lg mb-4">Experimental Configuration</h4>
    <p class="text-gray-700 leading-relaxed">
      Our experimental platform consists of a robotic arm equipped with a parallel-jaw gripper and
      an overhead RGB camera. A diverse set of household objects with varying shapes, sizes, and textures
      is used to evaluate system performance in cluttered and stacked scenes.
    </p>
  </div>
</section>

<!-- Fig 6 (replacing 8.png) -->
<section class="mt-10 text-center uniform-width">
  <div class="bg-white rounded-lg p-6 shadow-soft image-container">
    <img src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/6.png"
         class="w-full figure-img rounded"
         alt="Fig.6 Experimental Procedure">
    <p class="mt-2 text-sm text-gray-600">
      Real-world robotic grasping procedure in cluttered and stacked scenarios.
    </p>
    <p class="mt-4 text-gray-700 leading-relaxed text-justify">
      We conduct experiments with 2–5 objects under both cluttered and stacked configurations.
      For each configuration, multiple trials are performed to evaluate success rates and stability.
    </p>
  </div>
</section>

<!-- Results tables -->
<section id="results" class="mt-12 grid grid-cols-1 lg:grid-cols-2 gap-6 uniform-width">
  <div class="bg-white p-6 rounded-lg shadow-soft">
    <h4 class="font-semibold mb-4 text-center">Model Results</h4>
    <table class="w-full border">
      <thead>
      <tr>
        <th>Models</th>
        <th>Cluttered (%)</th>
        <th>Stacked (%)</th>
      </tr>
      </thead>
      <tbody>
      <tr><td>Mutli-Task CNN</td><td>90.60</td><td>65.65</td></tr>
      <tr><td>SMTNet</td><td>86.13</td><td>65.00</td></tr>
      <tr><td>EGNet</td><td>93.60</td><td>69.60</td></tr>
      <tr class="font-semibold bg-indigo-50"><td>D2TriPO-DETR (Ours)</td><td>95.71</td><td>74.29</td></tr>
      </tbody>
    </table>
  </div>

  <div class="bg-white p-6 rounded-lg shadow-soft">
    <h4 class="font-semibold mb-4 text-center">Per-object Success Rates</h4>
    <table class="w-full border">
      <thead>
      <tr><th>Objects</th><th>2</th><th>3</th><th>4</th><th>5</th></tr>
      </thead>
      <tbody>
      <tr><td>Cluttered</td><td>100%</td><td>100%</td><td>95%</td><td>92%</td></tr>
      <tr><td>Stacked</td><td>90%</td><td>73.33%</td><td>75%</td><td>68%</td></tr>
      </tbody>
    </table>
  </div>
</section>

<!-- Fig S3 & S4 -->
<section class="mt-10 grid grid-cols-1 md:grid-cols-2 gap-6 uniform-width">
  <div class="bg-white p-6 rounded-lg shadow-soft image-container">
    <img src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/S3.png"
         class="w-full figure-img rounded"
         alt="Fig.S3 Stability Evaluation">
    <p class="mt-2 text-sm text-gray-600">
      Stability evaluation under repeated experiments with varying object counts.
    </p>
  </div>

  <div class="bg-white p-6 rounded-lg shadow-soft image-container">
    <img src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/S4.png"
         class="w-full figure-img rounded"
         alt="Fig.S4 Boundary Ambiguity">
    <p class="mt-2 text-sm text-gray-600">
      Robust grasping performance under boundary-ambiguous scenarios.
    </p>
  </div>
</section>

<!-- Videos -->
<section id="video" class="mt-12 uniform-width">
  <h3 class="font-semibold text-xl text-center mb-6">Demonstration Videos</h3>

  <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
    <div class="bg-white p-4 rounded-lg shadow-soft">
      <video controls class="w-full rounded">
        <source src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/Mov.%201.mp4" type="video/mp4">
      </video>
      <p class="mt-2 text-sm text-gray-600 text-center">
        Stacked scene grasping experiments
      </p>
    </div>

    <div class="bg-white p-4 rounded-lg shadow-soft">
      <video controls class="w-full rounded">
        <source src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/Mov.%202.mp4" type="video/mp4">
      </video>
      <p class="mt-2 text-sm text-gray-600 text-center">
        Cluttered scene grasping experiments
      </p>
    </div>

    <div class="bg-white p-4 rounded-lg shadow-soft">
      <video controls class="w-full rounded">
        <source src="https://raw.githubusercontent.com/Embodied-Soft-Intelligence/TII/main/picture/Mov.%203.mp4" type="video/mp4">
      </video>
      <p class="mt-2 text-sm text-gray-600 text-center">
        Experiments for Special Cases Exceeding the Number of Stacked Objects in the Dataset
      </p>
    </div>
  </div>
</section>

<footer class="mt-12 text-center text-sm text-gray-500 uniform-width">
  © 2025 D2TriPO-DETR — Research demonstration
</footer>

</main>
</body>
</html>
